{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b55f8ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os, glob\n",
    "\n",
    "DATA_DIR = './data/icon/'\n",
    "FEATURES_DIR = './output/'\n",
    "VALID_EXTENSION = ['jpg', 'jpeg', 'JPG', 'JPEG', 'png', 'PNG', 'Png']\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2a600b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(net, self).__init__()\n",
    "        self.net = models.resnet50(pretrained=True)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        output = self.net.conv1(input)\n",
    "        output = self.net.bn1(output)\n",
    "        output = self.net.relu(output)\n",
    "        output = self.net.maxpool(output)\n",
    "        output = self.net.layer1(output)\n",
    "        output = self.net.layer2(output)\n",
    "        output = self.net.layer3(output)\n",
    "        output = self.net.layer4(output)\n",
    "        output = self.net.avgpool(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3fa1abcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        VGG = models.vgg16(pretrained=True)\n",
    "        self.feature = VGG.features\n",
    "        self.classifier = nn.Sequential(*list(VGG.classifier.children())[:-3])\n",
    "        pretrained_dict = VGG.state_dict()\n",
    "        model_dict = self.classifier.state_dict()\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "        model_dict.update(pretrained_dict)\n",
    "        self.classifier.load_state_dict(model_dict)\n",
    " \n",
    "    def forward(self, x):\n",
    "        output = self.feature(x)\n",
    "        output = output.view(output.size(0), -1)\n",
    "        output = self.classifier(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c73194ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = net()\n",
    "# model = Encoder()\n",
    "# model = model.cuda()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "910c07ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractor(img_path, saved_path, net, use_gpu):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor()]\n",
    "    )\n",
    "    \n",
    "    img = Image.open(img_path)\n",
    "    img = transform(img)\n",
    "    print('img shape: ', img.shape)\n",
    " \n",
    "    x = Variable(torch.unsqueeze(img, dim=0).float(), requires_grad=False)\n",
    "    print('x shape: ', x.shape)\n",
    " \n",
    "    if use_gpu:\n",
    "        x = x.cuda()\n",
    "        net = net.cuda()\n",
    "    y = net(x).cpu()\n",
    "    y = torch.squeeze(y)\n",
    "    y = y.data.numpy()\n",
    "    print(y.shape)\n",
    "    np.savetxt(saved_path, y, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "973fd3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessImage(img_path, save_dir):\n",
    "    \n",
    "    img = Image.open(img_path)\n",
    "#     img = img.convert('RGB')\n",
    "#     file_name = img_path.split('/')[-1]\n",
    "#     save_path = os.path.join(save_dir, file_name)\n",
    "#     img.save(save_path)\n",
    "    \n",
    "    tensorTransformer = transforms.ToTensor()\n",
    "    tensorImg = tensorTransformer(img)\n",
    "    print('path:{},  image shape: {}'.format(img_path, tensorImg.shape))\n",
    "    if img_path == './data/icon/c11_1.png':\n",
    "        print(tensorImg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aba15735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path:./data/icon/c6_2.png,  image shape: torch.Size([4, 10, 10])\n",
      "path:./data/icon/c4_1.png,  image shape: torch.Size([4, 39, 39])\n",
      "path:./data/icon/c6_3.png,  image shape: torch.Size([1, 32, 32])\n",
      "path:./data/icon/c6_1.png,  image shape: torch.Size([4, 12, 12])\n",
      "path:./data/icon/c4_2.png,  image shape: torch.Size([4, 20, 24])\n",
      "path:./data/icon/c2_1.png,  image shape: torch.Size([1, 64, 64])\n",
      "path:./data/icon/c2_2.png,  image shape: torch.Size([4, 64, 64])\n",
      "path:./data/icon/c7_1.png,  image shape: torch.Size([1, 22, 15])\n",
      "path:./data/icon/c5_2.png,  image shape: torch.Size([4, 16, 18])\n",
      "path:./data/icon/c7_2.png,  image shape: torch.Size([4, 16, 18])\n",
      "path:./data/icon/c7_3.png,  image shape: torch.Size([4, 20, 22])\n",
      "path:./data/icon/c5_1.png,  image shape: torch.Size([1, 48, 39])\n",
      "path:./data/icon/c1_1.png,  image shape: torch.Size([4, 64, 64])\n",
      "path:./data/icon/c9_8.png,  image shape: torch.Size([4, 48, 48])\n",
      "path:./data/icon/c7_7.png,  image shape: torch.Size([4, 16, 16])\n",
      "path:./data/icon/c7_6.png,  image shape: torch.Size([1, 24, 24])\n",
      "path:./data/icon/c9_9.png,  image shape: torch.Size([2, 44, 44])\n",
      "path:./data/icon/c3_2.png,  image shape: torch.Size([4, 28, 36])\n",
      "path:./data/icon/c1_2.png,  image shape: torch.Size([1, 64, 64])\n",
      "path:./data/icon/c7_4.png,  image shape: torch.Size([4, 20, 20])\n",
      "path:./data/icon/c7_5.png,  image shape: torch.Size([4, 16, 18])\n",
      "path:./data/icon/c3_1.png,  image shape: torch.Size([4, 40, 40])\n",
      "path:./data/icon/c9_7.png,  image shape: torch.Size([2, 30, 30])\n",
      "path:./data/icon/c7_8.png,  image shape: torch.Size([4, 18, 18])\n",
      "path:./data/icon/c7_9.png,  image shape: torch.Size([1, 22, 14])\n",
      "path:./data/icon/c9_6.png,  image shape: torch.Size([2, 68, 68])\n",
      "path:./data/icon/c9_4.png,  image shape: torch.Size([4, 10, 10])\n",
      "path:./data/icon/c9_5.png,  image shape: torch.Size([4, 36, 36])\n",
      "path:./data/icon/c9_12.png,  image shape: torch.Size([1, 40, 40])\n",
      "path:./data/icon/c9_1.png,  image shape: torch.Size([1, 36, 36])\n",
      "path:./data/icon/c9_13.png,  image shape: torch.Size([2, 16, 16])\n",
      "path:./data/icon/c10_1.png,  image shape: torch.Size([1, 22, 20])\n",
      "path:./data/icon/c9_11.png,  image shape: torch.Size([2, 44, 44])\n",
      "path:./data/icon/c9_2.png,  image shape: torch.Size([4, 36, 36])\n",
      "path:./data/icon/c9_3.png,  image shape: torch.Size([4, 36, 36])\n",
      "path:./data/icon/c9_10.png,  image shape: torch.Size([2, 24, 24])\n",
      "path:./data/icon/c10_2.png,  image shape: torch.Size([4, 30, 30])\n",
      "path:./data/icon/c11_4.png,  image shape: torch.Size([1, 28, 28])\n",
      "path:./data/icon/c8_2.png,  image shape: torch.Size([1, 48, 48])\n",
      "path:./data/icon/c11_3.png,  image shape: torch.Size([4, 60, 60])\n",
      "path:./data/icon/c11_2.png,  image shape: torch.Size([1, 64, 64])\n",
      "path:./data/icon/c8_3.png,  image shape: torch.Size([1, 48, 48])\n",
      "path:./data/icon/c8_1.png,  image shape: torch.Size([4, 48, 48])\n",
      "path:./data/icon/c11_1.png,  image shape: torch.Size([2, 34, 34])\n",
      "tensor([[[0.6784, 0.6784, 0.6784,  ..., 0.6784, 0.6784, 0.6784],\n",
      "         [0.6784, 0.6784, 0.6784,  ..., 0.6784, 0.6784, 0.6784],\n",
      "         [0.6784, 0.6784, 0.6784,  ..., 0.6784, 0.6784, 0.6784],\n",
      "         ...,\n",
      "         [0.6784, 0.6784, 0.6784,  ..., 0.6784, 0.6784, 0.6784],\n",
      "         [0.6784, 0.6784, 0.6784,  ..., 0.6784, 0.6784, 0.6784],\n",
      "         [0.6784, 0.6784, 0.6784,  ..., 0.6784, 0.6784, 0.6784]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "x = os.walk(DATA_DIR)\n",
    "for path, dirctory, filelist in x:\n",
    "    for filename in filelist:\n",
    "        file_glob = os.path.join(path, filename)\n",
    "        is_valid = False\n",
    "        for ext in VALID_EXTENSION:\n",
    "            if file_glob.endswith(ext):\n",
    "                is_valid = True\n",
    "                break\n",
    "        if is_valid:\n",
    "            preprocessImage(file_glob, FEATURES_DIR)\n",
    "#             print(file_glob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "83590c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./output/c6_2.png.txt\n",
      "img shape:  torch.Size([4, 224, 224])\n",
      "x shape:  torch.Size([1, 4, 224, 224])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 3, 7, 7], expected input[1, 4, 224, 224] to have 3 channels, but got 4 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/g2/jy_8z3fs28526pxbf3l1gbdm0000gn/T/ipykernel_76260/3706271514.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mfx_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFEATURES_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfx_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mextractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfx_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/g2/jy_8z3fs28526pxbf3l1gbdm0000gn/T/ipykernel_76260/2922943259.py\u001b[0m in \u001b[0;36mextractor\u001b[0;34m(img_path, saved_path, net, use_gpu)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/hoho/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/g2/jy_8z3fs28526pxbf3l1gbdm0000gn/T/ipykernel_76260/4060498877.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/hoho/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/hoho/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/hoho/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    440\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 442\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    443\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 3, 7, 7], expected input[1, 4, 224, 224] to have 3 channels, but got 4 channels instead"
     ]
    }
   ],
   "source": [
    "file_list = []\n",
    "x = os.walk(DATA_DIR)\n",
    "for path, directory, filelist in x:\n",
    "#     print(path)\n",
    "#     print(directory)\n",
    "#     print(filelist)\n",
    "#     print('---')\n",
    "    for filename in filelist:\n",
    "        file_glob = os.path.join(path, filename)\n",
    "        file_list.extend(glob.glob(file_glob))\n",
    "        \n",
    "# print(file_list)\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "for index, x_path in enumerate(file_list):\n",
    "    file_name = x_path.split('/')[-1]\n",
    "    fx_path = os.path.join(FEATURES_DIR, file_name + '.txt')\n",
    "    print(fx_path)\n",
    "    extractor(x_path, fx_path, model, use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dac2e0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177f02f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
